{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('../data/train.csv', dtype={'authorID': np.int64, 'h_index': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('../data/test.csv', dtype={'authorID': np.int64})\n",
    "n_test = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read collaboration graph\n",
    "G = nx.read_edgelist('../data/collaboration_network.edgelist', delimiter=' ', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read weighted collaboration graph\n",
    "WG = nx.read_edgelist(\"../data/weighted_collaboration_network.edgelist\", nodetype=int, data=((\"weight\", float),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read similarity graph\n",
    "SG = nx.read_multiline_adjlist(\"../data/sim_collaboration_network.adjlist\", nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {k: v for v, k in enumerate(list(G.nodes()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_neighbor_degree_sg = nx.average_neighbor_degree(SG)\n",
    "page_rank_sg = nx.pagerank(SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute graph features for each node\n",
    "avg_neighbor_degree_wg = nx.average_neighbor_degree(WG)\n",
    "avg_neighbor_degree_g = nx.average_neighbor_degree(G)\n",
    "core_number_g = nx.core_number(G)\n",
    "page_rank_g = nx.pagerank(G)\n",
    "page_rank_wg = nx.pagerank(WG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed features for each node\n",
    "f = open(\"../data/n_papers.pkl\", \"rb\")\n",
    "n_papers = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/average_coauthors_n_papers.pkl\", \"rb\")\n",
    "average_coauthors_n_papers = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/betweenness_centrality_g.pkl\", \"rb\")\n",
    "betweenness_centrality_g = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/betweenness_centrality_wg.pkl\", \"rb\")\n",
    "betweenness_centrality_wg = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/clustering_g.pkl\", \"rb\")\n",
    "clustering_g = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/clustering_wg.pkl\", \"rb\")\n",
    "clustering_wg = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../data/clustering_sg.pkl\", \"rb\")\n",
    "clustering_sg = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAE(nn.Module):\n",
    "    \"\"\"GAE model\"\"\"\n",
    "    def __init__(self, n_feat, n_hidden_1, n_hidden_2, n_hidden_3, n_hidden_4, dropout):\n",
    "        super(GAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_feat, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.fc4 = nn.Linear(n_hidden_3, n_hidden_4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj):\n",
    "        \n",
    "        h = self.fc1(x_in)\n",
    "        h = self.relu(torch.mm(adj, h))\n",
    "        \n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.relu(torch.mm(adj, h))\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        h = self.relu(torch.mm(adj, h))\n",
    "        \n",
    "        z = self.fc4(h)\n",
    "        z = torch.mm(adj, z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    A = A + sp.identity(n)\n",
    "    degs = A.dot(np.ones(n))\n",
    "    inv_degs = np.power(degs, -1)\n",
    "    D_inv = sp.diags(inv_degs)\n",
    "    A_normalized = D_inv.dot(A)\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "\n",
    "def sparse_to_torch_sparse(M):\n",
    "    \"\"\"Converts a sparse SciPy matrix to a sparse PyTorch tensor\"\"\"\n",
    "    M = M.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((M.row, M.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(M.data)\n",
    "    shape = torch.Size(M.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def loss_function(z, adj, device):\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    indices = adj._indices()\n",
    "\n",
    "    y_pred = list()\n",
    "    y = list()\n",
    "\n",
    "    y_pred.append(torch.sum(torch.mul(z[indices[0,:],:], z[indices[1,:],:]), dim=1))\n",
    "    y.append(adj._values().to(device))\n",
    "\n",
    "    random_indices = torch.randint(z.size(0), indices.size())\n",
    "    y_pred.append(torch.sum(torch.mul(z[random_indices[0,:],:], z[random_indices[1,:],:]), dim=1))\n",
    "    y.append(torch.zeros(random_indices.size(1)).to(device))\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    \n",
    "    loss = mse_loss(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read embeddings of abstracts\n",
    "text_embeddings = pd.read_csv(\"../data/author_embedding_64.csv\", header=None)\n",
    "text_embeddings = text_embeddings.rename(columns={0: \"authorID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = SG.number_of_nodes()\n",
    "nodes = {k: v for v, k in enumerate(list(G.nodes()))}\n",
    "features = np.zeros((n,64))\n",
    "for i in range(n):\n",
    "    author = text_embeddings[\"authorID\"][i]\n",
    "    index = nodes[author]\n",
    "    features[index,:] = text_embeddings.iloc[i, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 287.1659 time: 23.4365s\n",
      "Epoch: 0011 loss_train: 0.1116 time: 21.2820s\n",
      "Epoch: 0021 loss_train: 0.1077 time: 20.5230s\n",
      "Epoch: 0031 loss_train: 0.0973 time: 19.9500s\n",
      "Epoch: 0041 loss_train: 0.0854 time: 22.0450s\n",
      "Epoch: 0051 loss_train: 0.0747 time: 19.0360s\n",
      "Epoch: 0061 loss_train: 0.0658 time: 21.6300s\n",
      "Epoch: 0071 loss_train: 0.0584 time: 20.8820s\n",
      "Epoch: 0081 loss_train: 0.0523 time: 24.4342s\n",
      "Epoch: 0091 loss_train: 0.0466 time: 21.6174s\n",
      "(231239, 64)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_score\n",
    "\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 64\n",
    "n_hidden_3 = 64\n",
    "n_hidden_4 = 64\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "\n",
    "n = SG.number_of_nodes()\n",
    "adj = nx.adjacency_matrix(SG) # Obtains the adjacency matrix\n",
    "adj = normalize_adjacency(adj) # Normalizes the adjacency matrix\n",
    "\n",
    "# Transforms the numpy matrices/vectors to torch tensors\n",
    "features = torch.FloatTensor(features).to(device)\n",
    "adj = sparse_to_torch_sparse(adj).to(device)\n",
    "\n",
    "# Creates the model and specifies the optimizer\n",
    "model = GAE(features.shape[1], n_hidden_1, n_hidden_2, n_hidden_3, n_hidden_4, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Trains the model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(features, adj)\n",
    "    loss = loss_function(z, adj, device)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "graph_embeddings = z.detach().cpu().numpy()\n",
    "print(graph_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_temb = text_embeddings.shape[1] - 1\n",
    "n_gemb = graph_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training matrix. each node is represented as a vector of features:\n",
    "# (1-2-3) its degree, (4-5-6) the average degree of its neighbors, (7) its core number, \n",
    "# (8-9-10) its page rank, (11-12) its betweenness centrality, (13-14-15) its clustering coefficient,\n",
    "# (16) the number of written papers (cited), (17) the average number of written papers of its neighbors/coauthors,\n",
    "# (18) text_embeddings from Doc2Vec\n",
    "X_train = np.zeros((n_train, 17+n_temb+n_gemb))\n",
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    node = int(row['authorID'])\n",
    "    index = nodes[node]\n",
    "    X_train[i,0] = G.degree(node)\n",
    "    X_train[i,1] = WG.degree(node)\n",
    "    X_train[i,2] = SG.degree(node)\n",
    "    X_train[i,3] = avg_neighbor_degree_g[node]\n",
    "    X_train[i,4] = avg_neighbor_degree_wg[node]\n",
    "    X_train[i,5] = avg_neighbor_degree_sg[node]\n",
    "    X_train[i,6] = core_number_g[node]\n",
    "    X_train[i,7] = page_rank_g[node]\n",
    "    X_train[i,8] = page_rank_wg[node]\n",
    "    X_train[i,9] = page_rank_sg[node]\n",
    "    X_train[i,10] = betweenness_centrality_g[node]\n",
    "    X_train[i,11] = betweenness_centrality_wg[node]\n",
    "    X_train[i,12] = clustering_g[node]\n",
    "    X_train[i,13] = clustering_wg[node]\n",
    "    X_train[i,14] = clustering_sg[node]\n",
    "    X_train[i,15] = n_papers[node]\n",
    "    X_train[i,16] = average_coauthors_n_papers[node]\n",
    "    X_train[i,17:17+n_gemb] = graph_embeddings[index,:]\n",
    "    X_train[i,17+n_gemb:] = text_embeddings[text_embeddings[\"authorID\"] == node].iloc[:,1:]\n",
    "    y_train[i] = row['h_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LGBMRegressor(objective='mae', n_estimators=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.437653709750124\n"
     ]
    }
   ],
   "source": [
    "# cross-validation\n",
    "scores = cross_val_score(reg, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(np.mean(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the testing matrix. each node is represented as a vector of features:\n",
    "# (1-2-3) its degree, (4-5-6) the average degree of its neighbors, (7) its core number, \n",
    "# (8-9-10) its page rank, (11-12) its betweenness centrality, (13-14-15) its clustering coefficient,\n",
    "# (16) the number of written papers (cited), (17) the average number of written papers of its neighbors/coauthors,\n",
    "# (18) text_embeddings from Doc2Vec\n",
    "X_test = np.zeros((n_test, 17+n_temb+n_gemb))\n",
    "for i,row in df_test.iterrows():\n",
    "    node = int(row['authorID'])\n",
    "    index = nodes[node]\n",
    "    X_test[i,0] = G.degree(node)\n",
    "    X_test[i,1] = WG.degree(node)\n",
    "    X_test[i,2] = SG.degree(node)\n",
    "    X_test[i,3] = avg_neighbor_degree_g[node]\n",
    "    X_test[i,4] = avg_neighbor_degree_wg[node]\n",
    "    X_test[i,5] = avg_neighbor_degree_sg[node]\n",
    "    X_test[i,6] = core_number_g[node]\n",
    "    X_test[i,7] = page_rank_g[node]\n",
    "    X_test[i,8] = page_rank_wg[node]\n",
    "    X_test[i,9] = page_rank_sg[node]\n",
    "    X_test[i,10] = betweenness_centrality_g[node]\n",
    "    X_test[i,11] = betweenness_centrality_wg[node]\n",
    "    X_test[i,12] = clustering_g[node]\n",
    "    X_test[i,13] = clustering_wg[node]\n",
    "    X_test[i,14] = clustering_sg[node]\n",
    "    X_test[i,15] = n_papers[node]\n",
    "    X_test[i,16] = average_coauthors_n_papers[node]\n",
    "    X_test[i,17:17+n_gemb] = graph_embeddings[index,:]\n",
    "    X_test[i,17+n_gemb:] = text_embeddings[text_embeddings[\"authorID\"] == node].iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-processing: make sure that the predicted h-index is less than the number of papers (<10)\n",
    "for i in range(len(X_test)):\n",
    "    npapers = X_test[i, 12]\n",
    "    if npapers < 10 and y_pred[i] > npapers:\n",
    "        y_pred[i] = npapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the predictions to file\n",
    "df_test['h_index_pred'].update(pd.Series(np.round_(y_pred, decimals=3)))\n",
    "df_test.loc[:,[\"authorID\",\"h_index_pred\"]].to_csv('../predictions/test_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
